game:
  type: "stackelberg"

env:
  num_actions: 3
  board_size: 4
  reward: 1
  max_steps: 50
  init_defender_position: [0.,0., 0.]
  init_attacker_position: [3.,3.,4.712]
  goal_position: [0,0]
  goal_radius: 1.0
  capture_radius: 0.25
  timestep: 1
  velocity: 0.25
  turning_angle: 65
  reward_type: "-distance"
  training_method: "imbalanced, mixed attacker"
  

hyperparameters:
  learning_rate: 0.001
  gamma: 0.95
  epsilon_start: 0.001
  epsilon_end: 0.001
  epsilon_decay: 1

training:
  batch_multiple: 1
  num_parallel: 32
  num_episodes: 5000
  loaded_params: null
  

eval:
  eval_interval: 64
  num_eval_episodes: 128

#these are hard coded!
architecture:
  optimizer:
    optimizer: radam
  network:
    type: "mlp"
    hidden_layers: [100, 100, 100, 100]
    activation: "relu"
    output_activation: "linear"


#is try to ,make the bellman error high to start with bad unfavorable initial conditions
# isolote error of one player 
# red is attacker, blue is defender