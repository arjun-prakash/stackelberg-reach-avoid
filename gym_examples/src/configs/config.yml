game:
  type: "stackelberg"

env:
  num_actions: 3
  board_size: 4
  reward: 1
  max_steps: 50
  init_defender_position: [-3.5,0.,4.712]
  init_attacker_position: [0.,3.5, 0.] 
  goal_position: [0,-4]
  goal_radius: 1.0
  capture_radius: 0.5
  timestep: 1
  velocity: 0.25
  turning_angle: 65
  reward_type: "-soarse"
  training_method: "balanced, mixed attacker"
  note: "soft radius, penalised illegal moves."
  

hyperparameters:
  learning_rate: 0.001
  gamma: 0.95
  epsilon_start: 0.001
  epsilon_end: 0.001
  epsilon_decay: 1

training:
  batch_multiple: 1
  num_parallel: 32
  num_episodes: 5000
  loaded_params: None
  

eval:
  eval_interval: 64
  num_eval_episodes: 128

#these are hard coded!
architecture:
  optimizer:
    optimizer: radam betas .9 
  network:
    type: "mlp"
    hidden_layers: [100, 100, 100, 100]
    activation: "leaky_relu"
    output_activation: "linear"


#is try to ,make the bellman error high to start with bad unfavorable initial conditions
# isolote error of one player 
# red is attacker, blue is defender