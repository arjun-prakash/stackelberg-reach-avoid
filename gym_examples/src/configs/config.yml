game:
  type: "stackelberg"

env:
  num_actions: 3
  board_size: 4
  reward: 1
  max_steps: 50
  init_defender_position: [0.,-1,1.571] 
  init_attacker_position: [0,3.5, 4.712] 
  goal_position: [0,-4] 
  goal_radius: 1.0
  capture_radius: 0.25
  timestep: 1
  velocity: 0.25
  turning_angle: 65
  reward_type: "-dense each has own reward"
  training_method: "unbalanced (every 3 attacker, 1 defender), mixed attacker"
  note: "soft radius, penalised illegal moves. This one has no penalty for getting caught. attacker y is 3,2,1 can go back. attacker gets reward for winning"
  

hyperparameters:
  learning_rate: 0.001
  gamma: 0.99
  epsilon_start: 0.001
  epsilon_end: 0.001
  epsilon_decay: 1

training:
  batch_multiple: 1
  num_parallel: 32
  num_episodes: 2000
  loaded_params: None
  

eval:
  eval_interval: 128
  num_eval_episodes: 128

#these are hard coded!
architecture:
  optimizer:
    optimizer: radam betas .9 
  network:
    type: "mlp"
    hidden_layers: [64, 64]
    activation: "relu"
    output_activation: "linear"


#is try to ,make the bellman error high to start with bad unfavorable initial conditions
# isolote error of one player 
# red is attacker, blue is defender